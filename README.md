# WebAppEval: A Benchmark for Autonomous Web Agents and Agentic Test Automation

## Introduction

**WebAppEval** is a benchmark designed to evaluate **autonomous web agents** and **LLM-driven test automation systems**.  
It provides realistic, containerized web applications—such as **Magento**, **Odoo**, **Jorani**, and **Redmine**—and a standardized evaluation protocol that measures task performance in a reproducible and automated way.

While existing benchmarks like *MiniWoB++*, *Online-Mind2Web*, and *WebArena* have driven progress in this field, they often lack realism or reproducibility.  
**WebAppEval** fills this gap by combining:

- **Realistic enterprise-scale web apps**
- **JSON-based task schemas**
- **Automatic matchers for evaluation**
- **Unified scoring metrics (TSR, SCR)**
  **Reproducible Dockerized setup**
